{"cells":[{"cell_type":"markdown","id":"ba895618","metadata":{"id":"ba895618"},"source":["# Code 5: Building Damage Detection with Convolutional Neural Networks (CNNs)\n","\n","This code demonstrates the process of building damage detection using Convolutional Neural Networks (CNNs). It involves preprocessing images and annotations, creating and training a CNN model (based on VGG16), performing data augmentation, and utilizing selective search to identify and visualize regions of interest (potential damaged areas) in test images. The code showcases model training, evaluation, and testing for identifying whether an image contains signs of building damage."]},{"cell_type":"markdown","id":"35cc544a","metadata":{"id":"35cc544a"},"source":["## Part 1: Imports and Working directory"]},{"cell_type":"code","execution_count":null,"id":"f25af414","metadata":{"id":"f25af414"},"outputs":[],"source":["import os, cv2, keras\n","from cv2.ximgproc import segmentation\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","import keras\n","from keras.layers import Dense\n","from keras import Model\n","from keras import optimizers\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.optimizers import Adam\n","from keras.applications.vgg16 import VGG16\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.models import load_model\n","\n","path = \"path to train images\"\n","annot = \"path to corresponding annotations files\"\n"]},{"cell_type":"markdown","id":"f7c14e06","metadata":{"id":"f7c14e06"},"source":["## Part 2: Display Annotated Images"]},{"cell_type":"code","execution_count":null,"id":"8d4c53de","metadata":{"id":"8d4c53de"},"outputs":[],"source":["# Process the first ten files in the annotation directory.\n","for e, i in enumerate(os.listdir(annot)):\n","    if e < 10: # Limit the loop to the first ten files\n","        filename = i.split(\".\")[0] + \".jpg\"\n","        print(filename)\n","\n","        # Load the image and corresponding annotation data.\n","        img = cv2.imread(os.path.join(path, filename))\n","        df = pd.read_csv(os.path.join(annot, i))\n","\n","        # Display the original image.\n","        # plt.imshow(img)\n","\n","        # Iterate through annotation data and draw bounding boxes.\n","        for row in df.iterrows():\n","            x1 = int(row[1][4])\n","            y1 = int(row[1][5])\n","            x2 = int(row[1][6])\n","            y2 = int(row[1][7])\n","            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n","\n","        # Display the annotated image in a separate figure.\n","        # plt.figure()\n","        # plt.imshow(img)\n","\n","        # Exit the loop after processing the first image.\n","        break"]},{"cell_type":"markdown","id":"de4ca2e4","metadata":{"id":"de4ca2e4"},"source":["## Part 3: Selective Seach and Bounding Box"]},{"cell_type":"code","execution_count":null,"id":"191f513a","metadata":{"id":"191f513a"},"outputs":[],"source":["# Enable OpenCV optimizations for improved performance.\n","cv2.setUseOptimized(True);\n","\n","# Create an instance of the Selective Search Segmentation algorithm.\n","ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n","\n","# Load an image from the specified path.\n","im = cv2.imread(os.path.join(path, \"clip_1.jpg\"))\n","\n","# Set the base image for the Selective Search algorithm.\n","ss.setBaseImage(im)\n","\n","# Switch to the fast mode of Selective Search.\n","ss.switchToSelectiveSearchFast()\n","\n","# Perform Selective Search to generate region proposals.\n","rects = ss.process()\n","\n","# Create a copy of the image for visualization.\n","imOut = im.copy()\n","\n","# Iterate through the generated rectangles and draw bounding boxes.\n","for i, rect in (enumerate(rects)):\n","    x, y, w, h = rect\n","    #     print(x,y,w,h)\n","    #     imOut = imOut[x:x+w,y:y+h]\n","    cv2.rectangle(imOut, (x, y), (x + w, y + h), (0, 255, 0), 1, cv2.LINE_AA)\n","\n","# Display the image with drawn bounding boxes.\n","# plt.figure()\n","# plt.imshow(imOut)"]},{"cell_type":"markdown","id":"6cab3cd6","metadata":{"id":"6cab3cd6"},"source":["## Part 4: Data Preparation and Augmentation"]},{"cell_type":"code","execution_count":null,"id":"02ed128e","metadata":{"id":"02ed128e"},"outputs":[],"source":["# Initialize empty lists for storing training images and labels.\n","train_images = []\n","train_labels = []\n","\n","# Define a function to calculate Intersection over Union (IoU) between two bounding boxes.\n","def get_iou(bb1, bb2):\n","    \"\"\"\n","        Calculates the Intersection over Union (IoU) between two bounding boxes.\n","\n","        Args:\n","            bb1 (dict): Dictionary containing bounding box coordinates with keys 'x1', 'y1', 'x2', and 'y2'.\n","            bb2 (dict): Dictionary containing bounding box coordinates with keys 'x1', 'y1', 'x2', and 'y2'.\n","\n","        Returns:\n","            float: IoU value between 0 and 1, indicating the overlap between the two bounding boxes.\n","        \"\"\"\n","    # Assertions to validate input bounding boxes\n","    assert bb1['x1'] < bb1['x2']\n","    assert bb1['y1'] < bb1['y2']\n","    assert bb2['x1'] < bb2['x2']\n","    assert bb2['y1'] < bb2['y2']\n","\n","    # Calculate intersection area\n","    x_left = max(bb1['x1'], bb2['x1'])\n","    y_top = max(bb1['y1'], bb2['y1'])\n","    x_right = min(bb1['x2'], bb2['x2'])\n","    y_bottom = min(bb1['y2'], bb2['y2'])\n","\n","    # Check for non-overlapping bounding boxes\n","    if x_right < x_left or y_bottom < y_top:\n","        return 0.0\n","\n","    # Calculate IoU\n","    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n","    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n","    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n","    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n","\n","    # Ensure IoU value is within the valid range [0, 1]\n","    assert iou >= 0.0\n","    assert iou <= 1.0\n","    return iou\n","\n","def non_max_suppression(boxes, probs, overlap_thresh):\n","    \"\"\"\n","       Performs non-maximum suppression to filter out redundant bounding boxes.\n","\n","       Args:\n","           boxes (list): List of dictionaries representing bounding box coordinates.\n","           probs (list): List of confidence scores corresponding to the bounding boxes.\n","           overlap_thresh (float): Threshold for determining overlapping bounding boxes.\n","\n","       Returns:\n","           list: Indexes of the selected bounding boxes after non-maximum suppression.\n","       \"\"\"\n","    # Sort the bounding boxes by their confidence scores in descending order\n","    idxs = np.argsort(probs)[::-1]\n","\n","    # Initialize the list of picked indexes\n","    pick = []\n","\n","    while len(idxs) > 0:\n","        # Get the index of the highest confidence bounding box\n","        i = idxs[0]\n","\n","        # Add the index to the list of picked indexes\n","        pick.append(i)\n","\n","        # Calculate the overlap of this bounding box with others\n","        overlap = [get_iou(boxes[i], boxes[j]) for j in idxs[1:]]  # Calculate IOUs with other bounding boxes\n","\n","        # Remove indexes of overlapping bounding boxes\n","        idxs = idxs[np.where(np.array(overlap) <= overlap_thresh)[0] + 1]\n","\n","    return pick\n","\n","# Loop through the files in the annotation directory with enumeration.\n","for e, i in enumerate(os.listdir(annot)):\n","    try:\n","        # Check if the file name starts with \"clip\".\n","        if i.startswith(\"clip\"):\n","            filename = i.split(\".\")[0] + \".jpg\"\n","            print(e, filename)\n","\n","            # Read the image and annotation data.\n","            image = cv2.imread(os.path.join(path, filename))\n","            df = pd.read_csv(os.path.join(annot, i))\n","\n","            # Initialize a list to store ground truth bounding box values.\n","            gtvalues = []\n","\n","            # Extract ground truth bounding box coordinates from the DataFrame.\n","            for row in df.iterrows():\n","                for row in df.iterrows():\n","                    x1 = int(row[1][4])\n","                    y1 = int(row[1][5])\n","                    x2 = int(row[1][6])\n","                    y2 = int(row[1][7])\n","                gtvalues.append({\"x1\": x1, \"x2\": x2, \"y1\": y1, \"y2\": y2})\n","\n","            # Set the base image for Selective Search and perform fast mode segmentation.\n","            ss.setBaseImage(image)\n","            ss.switchToSelectiveSearchFast()\n","            ssresults = ss.process()\n","\n","            # Create a copy of the image for visualization.\n","            imout = image.copy()\n","\n","            # Initialize counters and flags. (Helps to manage the dataset generation process efficiently\n","            # while keeping a balance between positive and negative samples for training an object detection model)\n","            counter = 0\n","            falsecounter = 0\n","            flag = 0\n","            fflag = 0\n","            bflag = 0\n","\n","            # Loop through the selective search results and process bounding boxes.\n","            for e, result in enumerate(ssresults):\n","                if e < 2000 and flag == 0:\n","                    for gtval in gtvalues:\n","                        x, y, w, h = result\n","                        # Calculate IoU between result bounding box and ground truth.\n","                        iou = get_iou(gtval, {\"x1\": x, \"x2\": x + w, \"y1\": y, \"y2\": y + h})\n","\n","                        # Process bounding boxes for training dataset.\n","                        if counter < 30:\n","                            if iou > 0.70:\n","                                timage = imout[y:y + h, x:x + w]\n","                                resized = cv2.resize(timage, (224, 224), interpolation=cv2.INTER_AREA)\n","                                train_images.append(resized)\n","                                train_labels.append(1)\n","                                counter += 1\n","                        else:\n","                            fflag = 1\n","                        if falsecounter < 30:\n","                            if iou < 0.3:\n","                                timage = imout[y:y + h, x:x + w]\n","                                resized = cv2.resize(timage, (224, 224), interpolation=cv2.INTER_AREA)\n","                                train_images.append(resized)\n","                                train_labels.append(0)\n","                                falsecounter += 1\n","                        else:\n","                            bflag = 1\n","\n","                    # Set flags to exit the loops if necessary.\n","                    if fflag == 1 and bflag == 1:\n","                        print(\"inside\")\n","                        flag = 1\n","    except Exception as e:\n","        print(e)\n","        print(\"error in \" + filename)\n","        continue\n","\n","# Convert the list of training images and labels into a NumPy array.\n","X_new = np.array(train_images)\n","y_new = np.array(train_labels)\n","# Display the shape of the newly created training data array.\n","X_new.shape\n"]},{"cell_type":"markdown","id":"fe2ff898","metadata":{"id":"fe2ff898"},"source":["## Part 5: VGG16 Model and Training"]},{"cell_type":"code","execution_count":null,"id":"9f35f798","metadata":{"id":"9f35f798"},"outputs":[],"source":["\n","# Load VGG16 model with pre-trained weights\n","vggmodel = VGG16(weights='imagenet', include_top=True)\n","vggmodel.summary()\n","\n","# Freeze layers up to layer 15\n","for layers in vggmodel.layers[:15]:\n","    layers.trainable = False\n","\n","# Define custom output layer\n","X = vggmodel.layers[-2].output\n","predictions = Dense(2, activation=\"softmax\")(X)\n","\n","# Create the final model\n","model_final = Model(inputs=vggmodel.input, outputs=predictions)\n","\n","# Compile the model\n","opt = Adam(learning_rate=0.0001)\n","model_final.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt, metrics=[\"accuracy\"])\n","model_final.summary()\n","\n","\n","# function to binarize training data\n","class MyLabelBinarizer(LabelBinarizer):\n","    def transform(self, y):\n","        Y = super().transform(y)\n","        if self.y_type_ == 'binary':\n","            return np.hstack((Y, 1 - Y))\n","        else:\n","            return Y\n","\n","    def inverse_transform(self, Y, threshold=None):\n","        if self.y_type_ == 'binary':\n","            return super().inverse_transform(Y[:, 0], threshold)\n","        else:\n","            return super().inverse_transform(Y, threshold)\n","\n","\n","# apply Binarizer\n","lenc = MyLabelBinarizer()\n","Y = lenc.fit_transform(y_new)\n","\n","# split training data for testing and look at their dimensions\n","X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=0.20)\n","print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n","\n","\n","# Data augmentation\n","aug = ImageDataGenerator(\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    rotation_range=90,\n","    zoom_range=0.2,\n","    shear_range=0.2,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    fill_mode=\"nearest\"\n",")\n","\n","BS = 40\n","EPOCHS = 5\n","\n","# Train the model\n","print(\"[INFO] training head...\")\n","hist = model_final.fit(\n","    aug.flow(X_train, y_train, batch_size=BS),\n","    steps_per_epoch=len(X_train) // BS,\n","    validation_data=(X_test, y_test),\n","    validation_steps=len(X_test) // BS,\n","    epochs=EPOCHS\n",")\n","\n","\n","model.save(\"path to model.h5\")\n","model = keras.models.load_model(\"\")\n","\n"]},{"cell_type":"markdown","id":"5b075316","metadata":{"id":"5b075316"},"source":["## Part 6: Plotting and Testing"]},{"cell_type":"code","execution_count":null,"id":"abc516a3","metadata":{"id":"abc516a3"},"outputs":[],"source":["# plot the training loss and accuracy\n","N = EPOCHS\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), hist.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), hist.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, N), hist.history[\"accuracy\"], label=\"train_acc\")\n","plt.plot(np.arange(0, N), hist.history[\"val_accuracy\"], label=\"val_acc\")\n","plt.title(\"Training Loss and Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","plt.show()\n","plt.savefig(\"plot\")\n","\n","# test on test images\n","for im in X_test:\n","    img = np.expand_dims(im, axis=0)\n","    out = model.predict(img)\n","    if out[0][0] > out[0][1]:\n","        print(\"destroyed\")\n","    else:\n","        print(\"not destroyed\")\n"]},{"cell_type":"markdown","id":"cc5ec59f","metadata":{"id":"cc5ec59f"},"source":["## Part 7: Loading Model and Processing Images"]},{"cell_type":"code","execution_count":null,"id":"bb6f71cf","metadata":{"id":"bb6f71cf"},"outputs":[],"source":["\n","model_final = load_model(\"\")\n","# Initialize a counter to keep track of processed images.\n","z = 0\n","\n","# 3 clips kopiert und umbenannt zu \"test_\"\n","# Loop through the files in the directory 'path'.\n","for e, i in enumerate(os.listdir(path)):\n","    if i.startswith(\"test_\"):\n","        z += 1\n","        img = cv2.imread(os.path.join(path, i))\n","        ss.setBaseImage(img)\n","        ss.switchToSelectiveSearchFast()\n","        ssresults = ss.process() #Perform Selective Search on the image.\n","        imout = img.copy() #Create a copy of the image for drawing bounding boxes.\n","        boxes = []\n","\n","        # Iterate through the Selective Search results.\n","        for e, result in enumerate(ssresults):\n","            if e < 2000:\n","                x, y, w, h = result\n","                boxes.append([x, y, x + w, y + h])\n","\n","                # Extract the region of interest and resize it.\n","                timage = imout[y:y + h, x:x + w]\n","                resized = cv2.resize(timage, (224, 224), interpolation=cv2.INTER_AREA)\n","                img = np.expand_dims(resized, axis=0)\n","\n","        # Convert boxes to a NumPy array\n","        boxes = np.array(boxes)\n","\n","        # After predicting bounding boxes and their probabilities\n","        out = model_final.predict(img)\n","        proba = out[:, 0]\n","\n","        imout_copy = imout.copy()  # Create a copy for drawing boxes\n","\n","        # Apply Non-Maximum Suppression\n","        overlap_thresh = 0.3  # Adjust this threshold as needed\n","        nms_idxs = non_max_suppression(boxes, proba, overlap_thresh)\n","        filtered_boxes = boxes[nms_idxs]\n","\n","        # Draw rectangles for filtered bounding boxes\n","        for i in nms_idxs:\n","            x, y, w, h = filtered_boxes[i]\n","            cv2.rectangle(imout_copy, (x, y), (x + w, y + h), (0, 255, 0), 1, cv2.LINE_AA)\n","\n","        import matplotlib.pyplot as plt\n","\n","        # Save the image with rectangles\n","        plt.figure()\n","        plt.imshow(imout_copy)\n","\n","        # Generate a unique filename based on the iteration index (z)\n","        downloaded_filename = f\"path_{z}.png\"\n","\n","        # Save the image to the generated filename\n","        plt.savefig(downloaded_filename)\n","\n","        # Close the plot to prevent unwanted display\n","        plt.close()\n","\n","    print(\"Image processing and saving completed.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}